#!/usr/bin/env python
# /Users/seaking/Projects/tradewizard_4.1/src/llm_interpreter/run_single.py

import os
import sys

import asyncio
import argparse
import logging
import os
import sys
from typing import Dict, Any, Optional
from datetime import datetime, timezone
from dotenv import load_dotenv
import json
from supabase import create_client, Client
from src.modules.helpers import log_mcp_run, handle_mcp_result  # include patch helper
from src.llm_interpreter.interpreter import process_assessment
from src.llm_interpreter.output_formatter import format_mcp_results
from src.scrapers.playwright_crawler import PlaywrightCrawler
from postgrest.exceptions import APIError

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) # Corrected path
sys.path.insert(0, project_root)

from src.clients.supabase_client import get_supabase_client  # Added import

# Backwards compatibility stubs for missing imports
# process_single_assessment = process_assessment

async def run_single_assessment(record: dict) -> None:
    """Run WebsiteAnalysisModule and persist its output to mcp_runs."""
    from src.modules.website_analysis_module import WebsiteAnalysisModule

    module = WebsiteAnalysisModule()
    payload = await module.build_payload(record, [])
    mcp_output = await module.run(payload)

    # Log MCP run and apply its database patch
    log_mcp_run(
        mcp_name=module.NAME,  # Use .NAME instead of .name
        mcp_version=module.VERSION,  # Corrected to uppercase VERSION
        payload=payload,
        result=mcp_output.get("result"),
        confidence=mcp_output.get("confidence"),
        llm_input_prompt=mcp_output.get("llm_input_prompt"),
        llm_raw_output=mcp_output.get("llm_raw_output"),
        error=mcp_output.get("error"),
        classification_id=record["id"]
    )
    # Apply the database patch generated by the MCP (e.g., insert products)
    patch_applied_successfully = handle_mcp_result(mcp_output)

    if patch_applied_successfully:
        logger.info(f"Database patches applied successfully for Assessment {record['id']}.")
    else:
        logger.error(f"Failed to apply one or more database patches for Assessment {record['id']}. Check previous logs.")

    supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
    supabase_key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
    if not supabase_url or not supabase_key:
        logger.error("Supabase URL or Key not found in environment variables.")
        return
    supabase = create_client(supabase_url, supabase_key)
    
    supabase.table("Assessments").update({
        "llm_ready": False,
        "status": "completed"
    }).eq("id", record["id"]).execute()

def main():
    # --- Use argparse for proper argument handling ---
    parser = argparse.ArgumentParser(description="Run LLM analysis for a single assessment ID.")
    parser.add_argument("--assessment_id", required=True, help="The UUID of the assessment to process.")
    args = parser.parse_args()
    assessment_id = args.assessment_id
    # --- End of argparse setup ---

    # --- Fetch initial record ---
    record = fetch_record(assessment_id)
    if not record:
        sys.exit(1) # Error logged in fetch_record

    # --- Check for raw_content and crawl if necessary ---
    if not record.get("raw_content"):
        logger.info(f"Assessment {assessment_id}: raw_content missing. Initiating crawl.")
        source_url = record.get("source_url")
        if not source_url:
            logger.error(f"Assessment {assessment_id}: Cannot crawl, source_url is missing.")
            update_assessment_status(assessment_id, "failed", "Missing source URL for crawl")
            sys.exit(1)
        
        try:
            # Initialize and run the crawler
            # Consider making max_pages configurable via env var or assessment settings
            crawler = PlaywrightCrawler(max_pages=5) # Limit crawl depth for now
            logger.info(f"Assessment {assessment_id}: Starting Playwright crawl for URL: {source_url}")
            # crawl returns a dict, not a string
            crawl_results = asyncio.run(crawler.crawl(source_url))

            # Aggregate text content from all crawled pages
            aggregated_content = ""
            if crawl_results and "pages" in crawl_results:
                for page_data in crawl_results["pages"]:
                    if page_data.get("text"): 
                        # Add separator and URL for context (optional but helpful)
                        aggregated_content += f"\n\n--- Content from: {page_data.get('url', 'N/A')} ---\n\n"
                        aggregated_content += page_data["text"] 

            # Ensure we remove leading/trailing whitespace
            aggregated_content = aggregated_content.strip()

            if aggregated_content:
                logger.info(f"Assessment {assessment_id}: Crawl successful. Aggregated content length: {len(aggregated_content)}. Updating database.")
                # Update the database with the aggregated content
                if update_assessment_content(assessment_id, aggregated_content, "crawled"):
                    # Update the local record ONLY if DB update was successful
                    record["raw_content"] = aggregated_content
                    record["status"] = "crawled"
                else:
                    # If DB update failed, we cannot proceed reliably
                    logger.error(f"Assessment {assessment_id}: Failed to update database with crawled content. Halting processing.")
                    # Status was likely already set to 'failed' in update_assessment_content
                    sys.exit(1)
            else:
                logger.error(f"Assessment {assessment_id}: Crawl completed but resulted in empty aggregated content.")
                update_assessment_status(assessment_id, "failed", "Crawl yielded empty aggregated content")
                sys.exit(1)
        
        except Exception as e:
            logger.error(f"Assessment {assessment_id}: Crawling failed with error: {e}", exc_info=True)
            update_assessment_status(assessment_id, "failed", f"Crawler error: {e}")
            sys.exit(1)
    else:
        logger.info(f"Assessment {assessment_id}: raw_content already exists. Skipping crawl.")
    
    # --- Proceed with LLM analysis ---
    logger.info(f"Assessment {assessment_id}: Proceeding to LLM analysis.")
    asyncio.run(run_single_assessment(record)) # Pass the potentially updated record

def fetch_record(assessment_id: str) -> Optional[Dict[str, Any]]:
    supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
    supabase_key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
    if not supabase_url or not supabase_key:
        logger.error("Supabase URL or Key not found in environment variables.")
        return None
    supabase = create_client(supabase_url, supabase_key)
    
    response = supabase.table("Assessments").select("*").eq("id", assessment_id).execute()
    if not response.data:
        logger.error(f"Assessment with ID {assessment_id} not found.")
        return None
    return response.data[0]

def update_assessment_content(assessment_id: str, content: str, status: str):
    """Updates the raw_content and status for an assessment."""
    supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
    supabase_key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
    if not supabase_url or not supabase_key:
        logger.error("Supabase URL or Key not found in environment variables.")
        return False
    supabase = create_client(supabase_url, supabase_key)
    try:
        # Execute the update
        supabase.table("Assessments").update({
            "raw_content": content,
            "status": status,
            "updated_at": datetime.now(timezone.utc).isoformat() # Keep updated_at fresh
        }).eq("id", assessment_id).execute()
        
        # If execute() completed without raising an exception, assume success.
        # We know from logs the HTTP status code was 200 OK previously.
        logger.info(f"Assessment {assessment_id}: Successfully executed update for raw_content and status to '{status}'.")
        return True
    
    except Exception as e:
        logger.error(f"Assessment {assessment_id}: Exception during Supabase content update: {e}", exc_info=True)
        # Attempt to update status to failed ONLY if the primary update fails
        try:
            # Use the dedicated status update function if it exists, otherwise inline
            # Assuming update_assessment_status exists and handles its own errors
            from .supabase_utils import update_assessment_status # Assuming it's here
            update_assessment_status(assessment_id, "failed", f"DB content update error: {e}")
        except ImportError:
            # Fallback if update_assessment_status is not easily importable or defined elsewhere
            logger.warning("update_assessment_status not found, attempting inline status update.")
            try:
                supabase.table("Assessments").update({
                    "status": "failed",
                    "error_message": f"DB content update error: {e}",
                    "updated_at": datetime.now(timezone.utc).isoformat()
                }).eq("id", assessment_id).execute()
            except Exception as inner_e:
                logger.error(f"Assessment {assessment_id}: CRITICAL - Failed to update status to 'failed' after content update error: {inner_e}")
        except Exception as status_update_e:
              logger.error(f"Assessment {assessment_id}: CRITICAL - Failed during call to update_assessment_status after content update error: {status_update_e}")
        
        return False

def update_assessment_status(assessment_id: str, status: str, message: Optional[str] = None):
    """Updates the status and optionally a message for an assessment."""
    try:
        supabase = get_supabase_client()
        update_data = {
            "status": status,
            "updated_at": datetime.now(timezone.utc).isoformat(),
        }
        logger.info(f"Updating assessment {assessment_id} status to '{status}'")
        if message:
            logger.info(f"Status message (not saved to DB): {message}") # Log message instead

        response = supabase.table("Assessments").update(update_data).eq("id", assessment_id).execute()
        if not response.data:
            logger.warning(f"Assessment {assessment_id}: No data returned after status update. Record might not exist or RLS prevents view.")
            return False
        return True
    except Exception as e:
        logger.error(f"Assessment {assessment_id}: Exception during Supabase status update: {e}", exc_info=True)
        return False

def fetch_assessments_for_llm(*args, **kwargs):
    # Stub: not used in single-run
    return []

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

if __name__ == "__main__":
    main()
