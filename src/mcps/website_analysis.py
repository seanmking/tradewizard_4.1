# src/mcps/website_analysis.py

import json
import logging
from typing import Dict, Any, Optional, List
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from datetime import datetime, timezone

from .base import BaseMCP, MCPOutput
from llm_interpreter.llm_client import call_llm # Assuming LLM client is here
# Import crawler function if MCP triggers it directly (otherwise remove)
# from ..scrapers.crawler_integration import crawl_and_prepare_content

logger = logging.getLogger(__name__)

class WebsiteAnalysisMCP(BaseMCP):
    name = "website_analysis"
    version = "1.1.0" # Added version attribute for interpreter logging

    async def build_payload(self, assessment_data: Dict[str, Any], products: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Prepares payload, primarily ensuring necessary data like url and raw_content exist."""
        # This MCP primarily works off the raw_content generated by the crawler.
        # Payload building might be minimal here, just passing through necessary IDs/flags.
        return {
            "assessment_id": assessment_data.get("id"),
            "url": assessment_data.get("url"),
            "raw_content": assessment_data.get("raw_content"), # This should be the structured JSON
            "trigger_crawler": assessment_data.get("trigger_crawler", False) # Flag if crawl needed
        }

    async def run(self, payload: Dict[str, Any]) -> MCPOutput:
        """Analyzes website content (structured JSON) using an LLM."""
        assessment_id = payload.get("assessment_id")
        url = payload.get("url")
        raw_content_input = payload.get("raw_content")
        # Note: We assume trigger_crawler logic happens *before* this MCP runs,
        # and raw_content is populated correctly with the structured JSON.

        if not raw_content_input:
            logger.warning(f"Assessment {assessment_id}: No raw_content provided for website analysis.")
            return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error="Missing raw_content")

        # --- Step 1: Parse raw_content (assuming it might be a JSON string) ---
        try:
            if isinstance(raw_content_input, str):
                structured_content = json.loads(raw_content_input)
            elif isinstance(raw_content_input, dict): # Already a dict
                structured_content = raw_content_input
            else:
                raise ValueError("raw_content is not a valid JSON string or dictionary")
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Assessment {assessment_id}: Failed to parse raw_content JSON: {e}")
            return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error=f"Invalid raw_content format: {e}")

        # --- Step 2: Check Crawler Status & Handle Errors ---
        metadata = structured_content.get("metadata", {})
        crawl_status = metadata.get("crawl_status", "unknown")
        crawl_error = metadata.get("error")

        if crawl_status == "failed":
            logger.error(f"Assessment {assessment_id}: Crawler failed for {url}. Error: {crawl_error}. Halting WebsiteAnalysisMCP.")
            # Optionally update assessment status here via _db_patch
            return MCPOutput(
                assessment_id=assessment_id,
                mcp_name=self.name,
                status="error",
                error=f"Crawler failed: {crawl_error}"
                # Add db_patch here if you want to update the main assessment status to 'error'
                # _db_patch={"llm_status": "error", "llm_error": f"Crawler failed: {crawl_error}"}
            )
        
        analysis_warnings = []
        if crawl_status in ["partial", "completed_with_errors"]:
             warning_msg = f"Crawler finished with status '{crawl_status}'. Analysis may be based on incomplete data. Error: {crawl_error}"
             logger.warning(f"Assessment {assessment_id}: {warning_msg}")
             analysis_warnings.append(warning_msg)
        
        # --- Step 3: Extract Data for DB Patch --- 
        db_patch = {}
        assessment_updates = {} # Dictionary for updates to the main assessment record
        extracted_products = structured_content.get("aggregated_products", [])
        contact_info = metadata.get("contact_info", {})
        confidence_score = metadata.get("confidence_score")

        # Add data intended for insertion into extracted_products table
        if extracted_products:
            db_patch["extracted_products"] = extracted_products

        # Add data intended as updates to the main Assessments table
        if contact_info:
            assessment_updates["extracted_contacts"] = contact_info # Assuming 'extracted_contacts' column exists in Assessments
        if confidence_score is not None:
            assessment_updates["website_confidence_score"] = confidence_score # Assuming 'website_confidence_score' column exists

        # Add the assessment updates to the main patch under the correct table key
        # Note: We initialize assessment_updates in db_patch here, and add to it later if LLM succeeds
        if assessment_updates:
            db_patch["Assessments"] = { assessment_id: assessment_updates }
        else:
            # Ensure the structure exists even if initially empty, so we can add llm_summary later
            db_patch["Assessments"] = { assessment_id: {} }

        # --- Step 4: Prepare Input for LLM (using structured data) --- 
        # This part replaces the old `format_content_for_llm` logic.
        # It should create a textual summary/prompt for the LLM to generate insights (e.g., summary).
        try:
            llm_input_text = self._generate_llm_prompt_from_structured(structured_content, analysis_warnings)
        except Exception as e:
             logger.error(f"Assessment {assessment_id}: Error generating LLM prompt: {e}")
             # Decide whether to proceed without LLM or return error
             return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error=f"Prompt generation failed: {e}", _db_patch=db_patch) # Return patch with crawler data even if prompt fails

        # --- Step 5: Call LLM --- 
        llm_response_data = None # Initialize here
        llm_error = None
        try:
            # Assuming call_llm returns a parsed dictionary or raises an error
            logger.info(f"Assessment {assessment_id}: Calling LLM for website analysis.")
            # Example: Sending text prompt to LLM expecting JSON output for summary, etc.
            # Adjust model and parameters as needed
            llm_response_data = await call_llm(
                model="gpt-4-turbo-preview", # Or your preferred model
                prompt=llm_input_text,
                expected_format="json", # Correct parameter for JSON output
            )
            logger.debug(f"Assessment {assessment_id}: Raw LLM response data: {llm_response_data}") # Log raw response

            # --- Step 5b: Process LLM Response & Update Patch --- 
            if llm_response_data:
                # Safely extract summary
                llm_summary = llm_response_data.get("summary")
                if llm_summary:
                    logger.info(f"Assessment {assessment_id}: Parsed LLM summary: {llm_summary[:100]}...") # Log parsed summary (truncated)
                    # Add summary to the existing assessment updates
                    if assessment_id in db_patch.get("Assessments", {}):
                         db_patch["Assessments"][assessment_id]["llm_summary"] = llm_summary
                    else:
                        logger.warning(f"Assessment {assessment_id}: Could not find entry for assessment in db_patch['Assessments'] to add llm_summary.")
                else:
                     logger.warning(f"Assessment {assessment_id}: 'summary' key not found in LLM response.")

                # Safely extract products (overwriting crawler ones if LLM provides them)
                llm_products = llm_response_data.get("products")
                if llm_products is not None: # Check for None to allow empty list [] from LLM
                    if isinstance(llm_products, list):
                        logger.info(f"Assessment {assessment_id}: Using {len(llm_products)} products from LLM response.")
                        # Overwrite or add the extracted_products key
                        db_patch["extracted_products"] = llm_products
                    else:
                        logger.warning(f"Assessment {assessment_id}: 'products' key found in LLM response but is not a list: {type(llm_products)}")
                else:
                    logger.info(f"Assessment {assessment_id}: 'products' key not found or is null in LLM response. Retaining crawler products if any.")

                # Add other fields from LLM if needed (e.g., certifications, contacts)
                # Example:
                # llm_contacts = llm_response_data.get("contacts")
                # if llm_contacts and assessment_id in db_patch.get("Assessments", {}):
                #    db_patch["Assessments"][assessment_id]["extracted_contacts"] = llm_contacts # Decide if LLM overrides crawler contacts

            # Add LLM processing timestamp
            if assessment_id in db_patch.get("Assessments", {}):
                db_patch["Assessments"][assessment_id]["llm_processed_at"] = datetime.now(timezone.utc).isoformat()
            
            # Log the final db_patch before returning
            logger.debug(f"Assessment {assessment_id}: Final db_patch prepared: {db_patch}")

        except Exception as e:
            logger.error(f"Assessment {assessment_id}: Error during LLM call or processing: {e}", exc_info=True)
            llm_error = str(e)

        # --- Step 6: Construct Final MCP Output --- 
        status = "error" if llm_error else "completed"
        return MCPOutput(
            assessment_id=assessment_id,
            mcp_name=self.name,
            status=status,
            error=llm_error,
            output=llm_response_data, # Include parsed LLM output if available
            _db_patch=db_patch # Include the patch with potential crawler & LLM data
        )

    def _generate_llm_prompt_from_structured(self, content: Dict[str, Any], warnings: List[str]) -> str:
        """Generates a text prompt for the LLM based on the structured crawler output."""
        metadata = content.get("metadata", {})
        contact_info = metadata.get("contact_info", {})
        products = content.get("aggregated_products", [])
        pages = content.get("pages", [])
        
        prompt_lines = [
            "### Website Analysis Report ###",
            f"Source URL: {metadata.get('start_url', 'N/A')}",
            f"Crawl Status: {metadata.get('crawl_status', 'unknown')}",
            f"Confidence Score: {metadata.get('confidence_score', 0.0):.2f}",
        ]

        if warnings:
            prompt_lines.append("\nAnalysis Warnings:")
            for warning in warnings:
                 prompt_lines.append(f"- {warning}")

        prompt_lines.extend([
            "\n### Contact Information ###",
            f"Emails: {', '.join(contact_info.get('emails', ['None found']))}",
            f"Phones: {', '.join(contact_info.get('phones', ['None found']))}",
            # Address formatting needs improvement based on how it's stored
            f"Addresses: {'; '.join(contact_info.get('addresses', ['None found']))}", 
            f"Social Links: {', '.join(contact_info.get('social_links', ['None found']))}",
            "\n### Product Summary ###",
            f"Total Unique Products Found: {len(products)}",
        ])

        if products:
            prompt_lines.append("Sample Products (up to 10):")
            for i, product in enumerate(products[:10]):
                name = product.get('name', 'N/A')
                category = product.get('category', 'N/A')
                prompt_lines.append(f"  {i+1}. Name: {name}, Category: {category}")
        else:
             prompt_lines.append("No products were found by the crawler.")

        # Include snippets from key pages if available
        prompt_lines.append("\n### Key Page Content Snippets ###")
        key_page_count = 0
        for page in pages:
             url_lower = page.get('url','').lower()
             title_lower = page.get('title','').lower()
             # Identify key pages (simple example)
             if page.get('status') == 'success' and ('/' == urlparse(url_lower).path or 'about' in url_lower or 'contact' in url_lower or 'home' in title_lower):
                 key_page_count += 1
                 prompt_lines.append(f"--- Page: {page.get('url')} ({page.get('title', 'No Title')}) ---")
                 # Safely get text content if available
                 page_content = page.get('text_content', '') # Assumes text_content was added in crawler
                 if not page_content and page.get('content'): # Fallback to parsing raw HTML if text not stored
                      try:
                           page_content = BeautifulSoup(page.get('content'), 'html.parser').get_text(separator=' ', strip=True)
                      except Exception:
                           page_content = "(Could not parse HTML for text)"
                 
                 snippet = (page_content or "(No text content found)")[:300] # Limit snippet size
                 prompt_lines.append(snippet + "...")
                 if key_page_count >= 3: # Limit to 3 key pages
                     break
        if key_page_count == 0:
            prompt_lines.append("No key page content available.")

        prompt_lines.extend([
            "\n### Analysis Task ###",
            "Based on the information provided above:",
            "1. Generate a concise summary (2-3 sentences) of the company's likely business and primary offerings.",
            "2. Identify the main products or services offered. List them clearly.",
            "3. Briefly mention any notable aspects like certifications or target markets if evident in the text.",
            "4. Provide the output as a JSON object containing 'summary' (string) and 'products' (list of objects, each with 'name' and optional 'category' keys).",
            "Example JSON format: {\"summary\": \"This company sells...\", \"products\": [{\"name\": \"Product A\", \"category\": \"Category 1\"}, {\"name\": \"Service B\"}]}",
        ])

        return "\n".join(prompt_lines)

# Example usage (if running standalone for testing)
# async def test_mcp():
#     test_payload = {
#         "assessment_id": "test-123",
#         "url": "http://example.com",
#         "raw_content": json.dumps({
#             "metadata": { "start_url": "http://example.com", "crawl_status": "completed", "confidence_score": 0.8, "contact_info": {"emails": ["test@example.com"]}},
#             "pages": [],
#             "aggregated_products": [{"name": "Test Product"}]
#         })
#     }
#     mcp = WebsiteAnalysisMCP()
#     result = await mcp.run(test_payload)
#     print(json.dumps(result, indent=2))
#
# if __name__ == "__main__":
#     asyncio.run(test_mcp())