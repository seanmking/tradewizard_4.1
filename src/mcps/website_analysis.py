# src/mcps/website_analysis.py

import json
import logging
import os
from typing import Dict, Any, Optional, List
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from dotenv import load_dotenv # Add import

from .base import BaseMCP, MCPOutput, StandardizedMCPData
from llm_interpreter.llm_client import call_llm # Assuming LLM client is here
# Import crawler function if MCP triggers it directly (otherwise remove)
# from ..scrapers.crawler_integration import crawl_and_prepare_content

logger = logging.getLogger(__name__)

class WebsiteAnalysisMCP(BaseMCP):
    name = "website_analysis"
    version = "1.1.0" # Added version attribute for interpreter logging

    async def build_payload(self, assessment_data: Dict[str, Any], products: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Prepares payload, primarily ensuring necessary data like url and raw_content exist."""
        # This MCP primarily works off the raw_content generated by the crawler.
        # Payload building might be minimal here, just passing through necessary IDs/flags.
        return {
            "assessment_id": assessment_data.get("id"),
            "url": assessment_data.get("url"),
            "crawler_data": assessment_data.get("crawler_data"), # This should be the structured JSON
            "trigger_crawler": assessment_data.get("trigger_crawler", False) # Flag if crawl needed
        }

    async def run(self, payload: Dict[str, Any]) -> MCPOutput:
        """Analyzes website content (structured JSON) using an LLM."""
        load_dotenv() # Call load_dotenv at the start
        
        assessment_id = payload.get("assessment_id")
        url = payload.get("url")
        crawler_data_dict = payload.get("crawler_data") # Expecting a dict here now
        # Note: We assume trigger_crawler logic happens *before* this MCP runs,
        # and crawler_data is populated correctly with the structured JSON.

        if not crawler_data_dict:
            logger.warning(f"Assessment {assessment_id}: No crawler_data provided for website analysis.")
            return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error="Missing crawler_data", result={})

        # --- Step 1: Parse crawler_data (assuming it might be a JSON string) ---
        try:
            if isinstance(crawler_data_dict, str):
                # Attempt to parse if it's a string, keep as dict otherwise
                crawler_data_dict = json.loads(crawler_data_dict)
                if not isinstance(crawler_data_dict, dict):
                    raise ValueError("Parsed JSON is not a dictionary")
            elif isinstance(crawler_data_dict, dict): # Already a dict
                pass # Already a dict, no action needed
            else:
                raise ValueError("crawler_data is not a valid JSON string or dictionary")
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Assessment {assessment_id}: Failed to parse crawler_data JSON: {e}")
            return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error=f"Invalid crawler_data format: {e}", result={})

        # --- Step 2: Check Crawler Status & Handle Errors ---
        metadata = crawler_data_dict.get("metadata", {})
        crawl_status = metadata.get("crawl_status", "unknown")
        crawl_error = metadata.get("error")

        if crawl_status == "failed":
            logger.error(f"Assessment {assessment_id}: Crawler failed for {url}. Error: {crawl_error}. Halting WebsiteAnalysisMCP.")
            # Optionally update assessment status here via _db_patch
            db_patch = self._prepare_db_patch(assessment_id, crawler_data_dict, None) # Prepare patch even on crawl failure
            return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error=f"Crawler failed: {crawl_error}", _db_patch=db_patch, result={})
        
        analysis_warnings = []
        if crawl_status in ["partial", "completed_with_errors"]:
             warning_msg = f"Crawler finished with status '{crawl_status}'. Analysis may be based on incomplete data. Error: {crawl_error}"
             logger.warning(f"Assessment {assessment_id}: {warning_msg}")
             analysis_warnings.append(warning_msg)
        
        # --- Step 3: Extract Data for DB Patch --- 
        db_patch = {}
        assessment_updates = {} # Dictionary for updates to the main assessment record
        extracted_products = crawler_data_dict.get("aggregated_products", [])
        contact_info = metadata.get("contact_info", {})
        confidence_score = metadata.get("confidence_score")

        # Add data intended for insertion into extracted_products table
        if extracted_products:
            db_patch["extracted_products"] = extracted_products

        # Add data intended as updates to the main Assessments table
        if contact_info:
            assessment_updates["extracted_contacts"] = contact_info # Assuming 'extracted_contacts' column exists in Assessments
        if confidence_score is not None:
            assessment_updates["website_confidence_score"] = confidence_score # Assuming 'website_confidence_score' column exists

        # Add the assessment updates to the main patch under the correct table key
        # Note: We initialize assessment_updates in db_patch here, and add to it later if LLM succeeds
        if assessment_updates:
            db_patch["Assessments"] = { assessment_id: assessment_updates }
        else:
            # Ensure the structure exists even if initially empty, so we can add llm_summary later
            db_patch["Assessments"] = { assessment_id: {} }

        # --- Step 3: Check API Key --- 
        api_key = os.getenv('OPENAI_API_KEY') # Get key after load_dotenv
        if not api_key:
            logger.error(f"Assessment {assessment_id}: OpenAI API key is not configured.")
            # Ensure the db_patch reflects the 'pending' status correctly before returning
            db_patch["Assessments"][assessment_id]['llm_status'] = 'pending' 
            db_patch["extracted_products"] = [] # Ensure no products are added
            return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error="OpenAI API key not configured", _db_patch=db_patch, result={})

        # --- Step 4: Prepare Input for LLM (using structured data) --- 
        # This part replaces the old `format_content_for_llm` logic.
        # It should create a textual summary/prompt for the LLM to generate insights (e.g., summary).
        analysis_warnings = [] # Placeholder for warnings during this MCP's analysis (not crawler warnings)
        try:
            llm_input_text = self._generate_llm_prompt_from_structured(assessment_id, crawler_data_dict, analysis_warnings)
        except Exception as e:
            logger.error(f"Assessment {assessment_id}: Error generating LLM prompt: {e}", exc_info=True)
            return MCPOutput(assessment_id=assessment_id, mcp_name=self.name, status="error", error=f"Prompt generation failed: {e}", _db_patch=db_patch, result={}) # Return patch with crawler data even if prompt fails

        # --- Step 5: Call LLM --- 
        llm_response_data = None # Initialize here
        llm_error = None
        try:
            # Assuming call_llm returns a parsed dictionary or raises an error
            logger.info(f"Assessment {assessment_id}: Calling LLM for website analysis.")
            # Example: Sending text prompt to LLM expecting JSON output for summary, etc.
            # Adjust model and parameters as needed
            llm_response_data = await call_llm(
                model="gpt-4-turbo-preview", # Or your preferred model
                prompt=llm_input_text,
                expected_format="json", # Correct parameter for JSON output
            )
            logger.debug(f"Assessment {assessment_id}: Raw LLM response data: {llm_response_data}") # Log raw response

            # --- Step 5b: Process LLM Response & Update Patch --- 
            if llm_response_data:
                # Safely extract summary
                llm_summary = llm_response_data.get("summary")
                if llm_summary:
                    logger.info(f"Assessment {assessment_id}: Parsed LLM summary: {llm_summary[:100]}...") # Log parsed summary (truncated)
                    # Add summary to the existing assessment updates
                    if assessment_id in db_patch.get("Assessments", {}):
                         db_patch["Assessments"][assessment_id]["llm_summary"] = llm_summary
                    else:
                        logger.warning(f"Assessment {assessment_id}: Could not find entry for assessment in db_patch['Assessments'] to add llm_summary.")
                else:
                     logger.warning(f"Assessment {assessment_id}: 'summary' key not found in LLM response.")

                # Safely extract products (overwriting crawler ones if LLM provides them)
                llm_products = llm_response_data.get("products")
                if llm_products is not None: # Check for None to allow empty list [] from LLM
                    if isinstance(llm_products, list):
                        logger.info(f"Assessment {assessment_id}: Using {len(llm_products)} products from LLM response.")
                        # Overwrite or add the extracted_products key
                        db_patch["extracted_products"] = llm_products
                    else:
                        logger.warning(f"Assessment {assessment_id}: 'products' key found in LLM response but is not a list: {type(llm_products)}")
                else:
                    logger.info(f"Assessment {assessment_id}: 'products' key not found or is null in LLM response. Retaining crawler products if any.")

                # Add other fields from LLM if needed (e.g., certifications, contacts)
                # Example:
                # llm_contacts = llm_response_data.get("contacts")
                # if llm_contacts and assessment_id in db_patch.get("Assessments", {}):
                #    db_patch["Assessments"][assessment_id]["extracted_contacts"] = llm_contacts # Decide if LLM overrides crawler contacts

            # Add LLM processing timestamp
            if assessment_id in db_patch.get("Assessments", {}):
                db_patch["Assessments"][assessment_id]["llm_processed_at"] = datetime.now(timezone.utc).isoformat()
            
            # Log the final db_patch before returning
            logger.debug(f"Assessment {assessment_id}: Final db_patch prepared: {db_patch}")

        except Exception as e:
            logger.error(f"Assessment {assessment_id}: Error during LLM call or processing: {e}", exc_info=True)
            llm_error = str(e)

        # --- Step 6: Construct Final MCP Output --- 
        status = "error" if llm_error else "completed"
        try:
            final_db_patch = self._prepare_db_patch(assessment_id, crawler_data_dict, llm_response_data)
        except Exception as e:
             logger.error(f"Assessment {assessment_id}: Error preparing final DB patch: {e}", exc_info=True)
             # Decide: still return 'completed' but log error, or return 'error'? Let's log and complete.
        return MCPOutput(
            assessment_id=assessment_id,
            mcp_name=self.name,
            status=status,
            error=llm_error,
            result=llm_response_data, # Include parsed LLM output if available
            _db_patch=final_db_patch # Include the patch with potential crawler & LLM data
        )

    def _generate_llm_prompt_from_structured(self, assessment_id: str, crawler_data_dict: Dict[str, Any], warnings: List[str]) -> str:
        logger.debug(f"Entering _generate_llm_prompt_from_structured for assessment: {assessment_id}")
        """Generates a text prompt for the LLM based on the structured crawler data."""
        metadata = crawler_data_dict.get("metadata", {})
        contact_info = metadata.get("contact_info", {})
        products = crawler_data_dict.get("aggregated_products", [])
        pages = crawler_data_dict.get("pages", [])
         
        prompt_lines = [
            "### Website Analysis Report ###",
            f"Source URL: {metadata.get('start_url', 'N/A')}",
            f"Crawl Status: {metadata.get('crawl_status', 'unknown')}",
            f"Confidence Score: {metadata.get('confidence_score', 0.0):.2f}",
        ]

        if warnings:
            prompt_lines.append("\nAnalysis Warnings:")
            for warning in warnings:
                 prompt_lines.append(f"- {warning}")

        prompt_lines.extend([
            "\n### Contact Information ###",
            f"Emails: {', '.join(contact_info.get('emails', ['None found']))}",
            f"Phones: {', '.join(contact_info.get('phones', ['None found']))}",
            # Address formatting needs improvement based on how it's stored
            f"Addresses: {'; '.join(contact_info.get('addresses', ['None found']))}", 
            f"Social Links: {', '.join(contact_info.get('social_links', ['None found']))}",
            "\n### Product Summary ###",
            f"Total Unique Products Found: {len(products)}",
        ])

        if products:
            prompt_lines.append("Sample Products (up to 10):")
            for i, product in enumerate(products[:10]):
                name = product.get('name', 'N/A')
                category = product.get('category', 'N/A')
                prompt_lines.append(f"  {i+1}. Name: {name}, Category: {category}")
        else:
             prompt_lines.append("No products were found by the crawler.")

        # Include snippets from key pages if available
        prompt_lines.append("\n### Key Page Content Snippets ###")
        key_page_count = 0
        for page in pages:
             url_lower = page.get('url','').lower()
             title_lower = page.get('title','').lower()
             # Identify key pages (simple example)
             if page.get('status') == 'success' and ('/' == urlparse(url_lower).path or 'about' in url_lower or 'contact' in url_lower or 'home' in title_lower):
                 key_page_count += 1
                 prompt_lines.append(f"--- Page: {page.get('url')} ({page.get('title', 'No Title')}) ---")
                 # Safely get text content if available
                 page_content = page.get('text_content', '') # Assumes text_content was added in crawler
                 if not page_content and page.get('content'): # Fallback to parsing raw HTML if text not stored
                      try:
                           page_content = BeautifulSoup(page.get('content'), 'html.parser').get_text(separator=' ', strip=True)
                      except Exception:
                           page_content = "(Could not parse HTML for text)"
                 
                 snippet = (page_content or "(No text content found)")[:300] # Limit snippet size
                 prompt_lines.append(snippet + "...")
                 if key_page_count >= 3: # Limit to 3 key pages
                     break
        if key_page_count == 0:
            prompt_lines.append("No key page content available.")

        prompt_lines.extend([
            "\n### Analysis Task ###",
            "Based on the information provided above:",
            "1. Generate a concise summary (2-3 sentences) of the company's likely business and primary offerings.",
            "2. Identify the main products or services offered. List them clearly.",
            "3. Briefly mention any notable aspects like certifications or target markets if evident in the text.",
            "4. Provide the output as a JSON object containing 'summary' (string) and 'products' (list of objects, each with 'name' and optional 'category' keys).",
            "Example JSON format: {\"summary\": \"This company sells...\", \"products\": [{\"name\": \"Product A\", \"category\": \"Category 1\"}, {\"name\": \"Service B\"}]}",
        ])

        final_prompt = "\n".join(prompt_lines)
        logger.debug(f"Exiting _generate_llm_prompt_from_structured for assessment: {assessment_id}. Prompt length: {len(final_prompt)}")
        return final_prompt

    def _prepare_db_patch(self, assessment_id: str, crawler_data_dict: Optional[Dict[str, Any]], llm_analysis_data: Optional[Dict]) -> Dict:
        """ 
        Prepares the dictionary object used to patch the database Assessment record
        and potentially create related records (e.g., extracted_products).
        """
        patch = {"Assessments": {assessment_id: {}}, "extracted_products": []}
        assessment_patch = patch["Assessments"][assessment_id]
        assessment_patch['llm_status'] = 'pending' 

        # --- Include Crawler Data in Patch (if available) ---
        if crawler_data_dict: 
            metadata = crawler_data_dict.get("metadata", {})
            products = crawler_data_dict.get("aggregated_products", [])

            # Update assessment fields from metadata
            if metadata.get('title'): assessment_patch['site_title'] = metadata.get('title')
            if metadata.get('description'): assessment_patch['site_description'] = metadata.get('description')
            if metadata.get('contact_info'): assessment_patch['contact_info'] = metadata.get('contact_info')
            if metadata.get('confidence_score') is not None: assessment_patch['crawl_confidence_score'] = metadata.get('confidence_score')
            assessment_patch['crawl_status'] = metadata.get('crawl_status', 'unknown')

            # Add products extracted by the *crawler* initially. This might be overwritten by LLM products later.
            if products: # Use crawler products if available
                patch["extracted_products"] = products
            else:
                patch["extracted_products"] = [] # Ensure key exists

        # --- Include LLM Analysis Data in Patch (if available) ---
        if llm_analysis_data: 
            assessment_patch['llm_processed_at'] = datetime.now(timezone.utc).isoformat()
            assessment_patch['llm_status'] = 'completed'
            if llm_analysis_data.get('summary'):
                assessment_patch['llm_summary'] = llm_analysis_data['summary']
            if llm_analysis_data.get('confidence_score') is not None:
                assessment_patch['llm_confidence_score'] = llm_analysis_data['confidence_score']
            # Overwrite extracted_products with the list from LLM analysis
            patch["extracted_products"] = llm_analysis_data.get('products', []) 
        else:
            # If no LLM data, update status based on why (e.g., pending, skipped, error)
            # This status might have been set earlier (e.g., no API key -> pending)
            # If it's still 'pending' here, it means LLM wasn't called for other reasons.
            # We might need more context passed into this function to set a more specific 'skipped' reason.
            # For now, if it reaches here without llm_data, and status is still 'pending', let's mark it skipped.
            if assessment_patch['llm_status'] == 'pending':
                # Infer status based on crawler status if available, otherwise default to skipped
                crawler_status = assessment_patch.get('crawl_status', 'unknown')
                if crawler_status == 'failed':
                    assessment_patch['llm_status'] = 'skipped_crawler_failed'
                elif crawler_status == 'error':
                    assessment_patch['llm_status'] = 'skipped_crawler_error'
                else:
                    assessment_patch['llm_status'] = 'skipped' # Generic skipped if no LLM data
            # Ensure llm_processed_at is set even if skipped/error
            if 'llm_processed_at' not in assessment_patch:
                assessment_patch['llm_processed_at'] = datetime.now(timezone.utc).isoformat()

        return patch