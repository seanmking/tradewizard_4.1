{
  "tasks": [
    {
      "id": 1,
      "title": "Setup Project Structure and Dependencies",
      "description": "Initialize project directories, install dependencies, and set up configuration for Scrapy and Playwright.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "details": "Create a new project repository for the TradeWizard 4.1 web scraping component. Set up the basic folder structure including directories for scrapers, processors, storage, and API interfaces. Install required dependencies for web scraping (e.g., BeautifulSoup, Selenium, Scrapy), social media API access, and data processing libraries. Configure environment variables for API keys and credentials.",
      "testStrategy": "Verify all dependencies install correctly and the project structure follows best practices. Create a simple test script that imports all major dependencies to confirm they're working properly.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create Repository and Define Folder Structure",
          "description": "Initialize the TradeWizard 4.1 repository and establish the core directory structure for the web scraping component.",
          "status": "pending",
          "dependencies": [],
          "details": "Create a new Git repository named 'tradewizard-4.1-scraper'. Set up the following directory structure: /scrapers (with subdirectories for different data sources), /processors (for data transformation logic), /storage (for database connectors and caching), /api (for external API interfaces), /tests (for unit and integration tests), and /config (for configuration files). Include a README.md with project overview and setup instructions."
        },
        {
          "id": 2,
          "title": "Install Core Web Scraping Dependencies",
          "description": "Set up the Python environment and install primary web scraping libraries needed for the project.",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Create a virtual environment using venv or conda. Install and configure the following core dependencies: BeautifulSoup4 for HTML parsing, Selenium with appropriate webdrivers for dynamic content, Scrapy for structured crawling, Requests for HTTP operations, and lxml for XML/HTML processing. Create a requirements.txt file documenting all dependencies with specific versions. Include a setup.py file for package installation."
        },
        {
          "id": 3,
          "title": "Implement Social Media API Connectors",
          "description": "Set up the necessary libraries and authentication for accessing social media platforms' APIs.",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "Install Python libraries for major social media platforms: tweepy for Twitter, facebook-sdk for Facebook, praw for Reddit, and python-linkedin for LinkedIn. Create connector classes in the /api directory that handle authentication and basic data retrieval for each platform. Implement proper error handling and rate limit management in each connector. Document the required API credentials for each platform."
        },
        {
          "id": 4,
          "title": "Configure Environment and Credentials Management",
          "description": "Establish a secure system for managing API keys, credentials, and environment-specific configurations.",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Create a .env.example file listing all required environment variables without actual values. Implement a configuration module using python-dotenv to load variables from .env files. Set up different configuration profiles for development, testing, and production environments. Add appropriate entries to .gitignore to prevent credentials from being committed. Create a documentation file explaining how to obtain and configure the necessary API keys for each service."
        },
        {
          "id": 5,
          "title": "Set Up Data Processing and Storage Libraries",
          "description": "Install and configure libraries for data processing, transformation, and storage of scraped information.",
          "status": "pending",
          "dependencies": [
            2,
            4
          ],
          "details": "Install pandas and numpy for data manipulation, SQLAlchemy for database ORM, pymongo for NoSQL storage options, and redis for caching. Create base classes in the /storage directory for different storage backends. Implement data models and schemas for structured storage of scraped data. Set up configuration for connecting to development databases locally. Include basic data validation utilities and transformation helpers in the /processors directory."
        }
      ]
    },
    {
      "id": 2,
      "title": "Implement SME Website Scraping (Scrapy)",
      "description": "Develop a Scrapy-based service for crawling SME company websites (e.g., www.kelvinspices.co.za) to extract products, About Us, company metadata, social links, and contact info.",
      "status": "pending",
      "dependencies": [1],
      "priority": "high",
      "details": "Implement Scrapy spiders for 5-10 provided SME company websites. Extract product listings, About Us section, company type/services/location, social links, and contact info. Output JSON in the schema required by the MCP and UI integration. Ensure all operations are scoped by assessmentId.",
      "testStrategy": "Test each spider against real SME sites, validate that all required sections are extracted and mapped to the JSON schema. Handle anti-bot/captcha gracefully. Test assessmentId scoping.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement Product & Metadata Extraction",
          "description": "Extract product names, descriptions, images, About Us, company metadata, social links, and contact info from SME websites.",
          "status": "pending",
          "dependencies": [],
          "details": "Create robust selectors for each target site. Map all data to the unified JSON structure for Supabase storage (scoped by assessmentId) and MCP consumption."
        },
        {
          "id": 2,
          "title": "REST API Output for Next.js Integration",
          "description": "Expose Scrapy results via REST endpoint for ingestion by the TypeScript backend.",
          "status": "pending",
          "dependencies": [1],
          "details": "Ensure output is MCP-compatible JSON, ready for UI and LLM prompt flow. Endpoint must accept assessmentId to fetch/trigger relevant data. Implement enhanced ScrapingError structure for API responses."
        }
      ]
    },
    {
      "id": 3,
      "title": "Implement Social Platform Scraping (Playwright)",
      "description": "Use Playwright (Node.js) to scrape LinkedIn, Instagram, Facebook, and Twitter/X for SME company and product data.",
      "status": "pending",
      "dependencies": [1],
      "priority": "high",
      "details": "Implement Playwright scripts to extract company profiles, product mentions, About/description, and engagement metrics from LinkedIn, Instagram, Facebook, and Twitter/X. Output structured JSON (scoped by assessmentId) as per the MCP and UI requirements. No Reddit. Implement enhanced ScrapingError structure for error reporting.",
      "testStrategy": "Test scraping against real company profiles. Validate output accuracy, structure, and error handling for anti-bot/CAPTCHA scenarios. Test assessmentId scoping.",
      "subtasks": [
        {
          "id": 1,
          "title": "LinkedIn Company Scraper",
          "description": "Extract company profile, about, products/services, and social links from LinkedIn.",
          "status": "pending",
          "dependencies": [],
          "details": "Use Playwright to navigate and extract relevant sections. Map to JSON schema. Scope data by assessmentId."
        },
        {
          "id": 2,
          "title": "Instagram Scraper",
          "description": "Extract product posts, company bio, and contact links from Instagram.",
          "status": "pending",
          "dependencies": [],
          "details": "Focus on posts mentioning products/services. Extract images, captions, and links. Scope data by assessmentId."
        },
        {
          "id": 3,
          "title": "Facebook Scraper",
          "description": "Extract company page details, products, about, and contact info from Facebook.",
          "status": "pending",
          "dependencies": [],
          "details": "Extract posts, about section, and social/contact links. Scope data by assessmentId."
        },
        {
          "id": 4,
          "title": "Twitter/X Scraper",
          "description": "Extract tweets mentioning products, company bio, and engagement metrics from Twitter/X.",
          "status": "pending",
          "dependencies": [],
          "details": "Extract relevant tweets, bio, and links. Map all data to unified JSON structure. Scope data by assessmentId."
        }
      ]
    },
    {
      "id": 4,
      "title": "Implement Frontend Context and UI Integration",
      "description": "Create React Context (ScrapingProvider/useScraping) for managing scraped data state and integrate with UI panels.",
      "status": "pending",
      "dependencies": [2, 3],
      "priority": "critical",
      "details": "Implement ScrapingProvider accepting assessmentId and Supabase client. Use Supabase real-time subscriptions for data refresh after startNewScrape. Implement enhanced ScrapingError type. Handle initial/empty state UX (Sarah prompt or UI fallback). Integrate context data with Center Panel, Right Panel, and Sarah prompt triggers.",
      "testStrategy": "Unit test context provider/hook. Integration test UI updates based on context state changes (loading, error, data, initial state). Verify real-time updates work correctly. Test error display and initial state prompts.",
      "subtasks": [
        {
          "id": 1,
          "title": "Develop Scraping Context Provider & Hook",
          "description": "Implement ScrapingProvider and useScraping hook with assessmentId scoping, Supabase real-time subscription logic, and enhanced error handling.",
          "status": "pending",
          "dependencies": [],
          "details": "Define IScrapingContext, ScrapingError interfaces. Implement state logic for loading, error, data, and isInitialState."
        },
        {
          "id": 2,
          "title": "Integrate Context with UI Panels",
          "description": "Connect Center Panel, Right Panel, and relevant action buttons to the useScraping hook.",
          "status": "pending",
          "dependencies": [1],
          "details": "Display scraped data, loading indicators, error messages (using resolutionHint), and handle initial state prompts."
        },
        {
          "id": 3,
          "title": "Integrate Context with Sarah Agent",
          "description": "Ensure Sarah agent logic can access scraped data from the context to inform prompts.",
          "status": "pending",
          "dependencies": [1],
          "details": "Pass scrapedData (especially products) to prompt generation logic. Trigger prompts based on data availability/updates."
        }
      ]
    },
    {
      "id": 5,
      "title": "Refactor Product Classification Flow",
      "description": "Overhaul the product classification experience in TradeWizard to use a stepwise, incremental UX flow (Grouping -> HS Code -> Compliance -> Review). Focus on reducing cognitive load and guiding the user through complex data entry.",
      "status": "in_progress",
      "dependencies": [],
      "priority": "high",
      "details": "Finalize data model for per-product and per-variant fields (export compliant). Design UI mockups for each step (Grouping, HS Code, Compliance, Review). Implement Step 1: Product Grouping & Variant Selection UI. Implement Step 2: HS Code Cascade/Classification (using Github repo data lookup). Implement Step 3: Compliance Fields (collapsible/optional sections). Implement Step 4: Review/Export screen (placeholder for now). Connect UI components to backend data model/state management. Add validation logic for each step. Ensure smooth transitions between steps. Write unit/integration tests for the new flow",
      "testStrategy": "",
      "subtasks": [
        {
          "id": 1,
          "title": "Finalize data model for per-product and per-variant fields (export compliant)",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 2,
          "title": "Design UI mockups for each step (Grouping, HS Code, Compliance, Review)",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 3,
          "title": "Implement Step 1: Product Grouping & Variant Selection UI",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 4,
          "title": "Implement Step 2: HS Code Cascade/Classification (using Github repo data lookup)",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 5,
          "title": "Implement Step 3: Compliance Fields (collapsible/optional sections)",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 6,
          "title": "Implement Step 4: Review/Export screen (placeholder for now)",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 7,
          "title": "Connect UI components to backend data model/state management",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 8,
          "title": "Add validation logic for each step",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 9,
          "title": "Ensure smooth transitions between steps",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 10,
          "title": "Write unit/integration tests for the new flow",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement LLM Interpreter for SME Assessment",
      "description": "Develop the LLM-based module to process scraped SME assessment data, extract structured information, and update Supabase.",
      "status": "in_progress",
      "dependencies": [
        5
      ],
      "priority": "high",
      "details": "Confirm correct fields are extracted from Supabase (`id`, `raw_content`, `is_mock`, etc.). Design and test prompt template for the LLM (include per-field confidence). Implement LLM interpreter function (OpenAI or Anthropic via `.env` keys). **Add pre-checks: skip if `is_mock=true` or `raw_content` length < 1000 chars.** Parse response into structured MCPData format. Write parser validation logic and field-level confidence handling. Upsert structured results into the same Supabase record (update by `id`). Mark `llm_ready = false` and `llm_processed_at` timestamp. Handle LLM or Supabase errors gracefully with retry logic and update `fallback_reason`. **Log extensively: assessment ID, mock status, content length, confidence scores, success/failure/skip reason.** Write unit test with mock LLM response. Document how the interpreter connects to Sarah and downstream UX",
      "testStrategy": "Unit test the interpreter function with sample raw content and mocked LLM/Supabase responses. Verify correct parsing, error handling, and Supabase update payloads. Integration test with live Supabase data (marked as mock). End-to-end test the flow from scraping to LLM processing.",
      "subtasks": [
        {
          "id": 1,
          "title": "Fetch Assessments where `llm_ready = true`",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Query Supabase for records meeting criteria (llm_ready=true, raw_content not null, is_mock=false)."
        },
        {
          "id": 2,
          "title": "Design & Test LLM Prompt Template",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Create a robust prompt instructing the LLM on JSON structure and field confidence."
        },
        {
          "id": 3,
          "title": "Implement LLM Interpreter Function",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Function to call the chosen LLM API (OpenAI/Anthropic) using credentials from .env."
        },
        {
          "id": 4,
          "title": "Implement Pre-Checks (is_mock, content length)",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Add checks to skip processing if record is mock or content too short."
        },
        {
          "id": 5,
          "title": "Parse LLM Response to MCPData",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Validate and parse the LLM's JSON output into the target MCPData structure. **Implement schema validation (required fields, types, value ranges) and confidence threshold checks.**"
        },
        {
          "id": 6,
          "title": "Upsert Structured MCPData to Supabase",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Update the original Supabase record by ID with the extracted structured data."
        },
        {
          "id": 7,
          "title": "Mark `llm_ready = false` and `llm_processed_at` timestamp",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Update status flags in Supabase upon successful processing."
        },
        {
          "id": 8,
          "title": "Handle Errors and Log",
          "description": "",
          "status": "complete",
          "dependencies": [],
          "details": "Implement error handling for API calls and database updates; add comprehensive logging. **Implement retry logic (e.g., exponential backoff) for transient errors.**"
        },
        {
          "id": 9,
          "title": "Write Unit Tests",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        },
        {
          "id": 10,
          "title": "Document how the interpreter connects to Sarah and downstream UX",
          "description": "",
          "status": "pending",
          "dependencies": [],
          "details": ""
        }
      ]
    }
  ],
  "metadata": {
    "projectName": "TradeWizard Web & Social Scraping Implementation",
    "totalTasks": 12,
    "sourceFile": "docs/web_scraper_implementation_plan.md",
    "generatedAt": "2023-11-09"
  }
}